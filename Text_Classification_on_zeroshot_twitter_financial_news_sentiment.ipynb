{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification on zeroshot/twitter-financial-news-sentiment from Hugging Face"
      ],
      "metadata": {
        "id": "_Ypu2jY1Ov5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "4iNpIgk1OlYO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGpYBDnu2YZW"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset by name\n",
        "dataset = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")"
      ],
      "metadata": {
        "id": "d44ks_b72agd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataset to a directory\n",
        "dataset.save_to_disk(\"zeroshot/twitter-financial-news-sentiment\")"
      ],
      "metadata": {
        "id": "Hm_2wGY12elx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the dataset from the saved directory\n",
        "dataset = load_from_disk(\"zeroshot/twitter-financial-news-sentiment\")"
      ],
      "metadata": {
        "id": "hLdNFR912jNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few examples from the dataset\n",
        "for example in dataset[\"train\"][:5]:\n",
        "    print(example)"
      ],
      "metadata": {
        "id": "Xa_AEf6s2lV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few examples from the dataset\n",
        "for example in dataset[\"validation\"][:5]:\n",
        "    print(example)"
      ],
      "metadata": {
        "id": "sJVT_7cY3aw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access a specific dataset within the DatasetDict\n",
        "specific_dataset = dataset[\"train\"]\n",
        "\n",
        "# Print basic information about the specific dataset\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(\"Number of examples:\", len(specific_dataset))\n",
        "print(\"Features available:\", specific_dataset.features)"
      ],
      "metadata": {
        "id": "CtM5N7C47S_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few examples\n",
        "print(\"\\nFirst few examples:\")\n",
        "for i in range(min(5, len(dataset[\"train\"]))):\n",
        "    example = dataset[\"train\"][i]\n",
        "    print(\"Example\", i+1, \":\")\n",
        "    print(\"Text:\", example[\"text\"])\n",
        "    print(\"Label:\", example[\"label\"])\n",
        "    print()"
      ],
      "metadata": {
        "id": "zggmiCNB79gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access a specific dataset within the DatasetDict\n",
        "specific_dataset = dataset[\"validation\"]\n",
        "\n",
        "# Print basic information about the specific dataset\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(\"Number of examples:\", len(specific_dataset))\n",
        "print(\"Features available:\", specific_dataset.features)"
      ],
      "metadata": {
        "id": "HURD50Il8DW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few examples\n",
        "print(\"\\nFirst few examples:\")\n",
        "for i in range(min(5, len(dataset[\"validation\"]))):\n",
        "    example = dataset[\"train\"][i]\n",
        "    print(\"Example\", i+1, \":\")\n",
        "    print(\"Text:\", example[\"text\"])\n",
        "    print(\"Label:\", example[\"label\"])\n",
        "    print()"
      ],
      "metadata": {
        "id": "DdxkuSOV8lfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "ibwxKSQh3BQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the training and validation splits\n",
        "train_data = dataset[\"train\"]\n",
        "val_data = dataset[\"validation\"]"
      ],
      "metadata": {
        "id": "O-Ifop1m9Dmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further split the validation data into validation and test sets\n",
        "val_data, test_data = train_test_split(val_data, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "ENA-b2639HLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing:\n",
        "\n",
        "Tokenization: The text data is split into individual words using CountVectorizer."
      ],
      "metadata": {
        "id": "ss8LSno2J7Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "y0TaK8di9nnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Conversion: Tokenized data is transformed into a format suitable for training by creating sparse matrices (X_train, X_val, X_test) of token counts."
      ],
      "metadata": {
        "id": "7UEcFISLKL0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectorizer.fit_transform(train_data[\"text\"])\n",
        "y_train = train_data[\"label\"]\n",
        "X_val = vectorizer.transform(val_data[\"text\"])\n",
        "y_val = val_data[\"label\"]\n",
        "X_test = vectorizer.transform(test_data[\"text\"])\n",
        "y_test = test_data[\"label\"]"
      ],
      "metadata": {
        "id": "Uu5NA48p9qd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training:\n",
        "\n",
        "Classification Model: Multinomial Naive Bayes (MultinomialNB) is chosen for classification.\n",
        "\n",
        "Pipeline: A scikit-learn pipeline (make_pipeline) integrates tokenization and model training.\n",
        "\n"
      ],
      "metadata": {
        "id": "xRAbEaldKV79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = make_pipeline(MultinomialNB())"
      ],
      "metadata": {
        "id": "oGvZOIYR9tCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training:\n",
        "\n",
        "The pipeline is trained on the training data (X_train, y_train) using the fit() method."
      ],
      "metadata": {
        "id": "0Wl2hoINKkc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "AKpI4jra9v-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation:\n",
        "\n",
        "Evaluation: The trained model is evaluated on the test set (X_test) to assess its performance.\n",
        "\n",
        "Metrics: Accuracy and F1 score (weighted average for multiclass classification) are computed to measure performance."
      ],
      "metadata": {
        "id": "tTibkp1bKrxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "OmlCNAQY9zBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Metrics: Accuracy and F1 score (weighted average for multiclass classification) are computed to measure performance"
      ],
      "metadata": {
        "id": "5yBD98qPK2ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' average for multiclass classification\n",
        "\n",
        "print(\"Model evaluation results:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "nykq1erG916V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Accuracy: The proportion of correctly classified instances among all instances. In this case, it means that about 78.81% of the tweets were correctly classified by the model.\n",
        "\n",
        "F1 Score: The weighted average of precision and recall. It is a harmonic mean of precision and recall and provides a balance between them. A higher F1 score indicates better performance, considering both false positives and false negatives."
      ],
      "metadata": {
        "id": "wf5HmyrNK6ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " let's explore different types of feature engineering and model selection techniques to potentially improve the performance of model.\n",
        "\n",
        " Implementing TF-IDF vectorization along with a Naive Bayes classifier"
      ],
      "metadata": {
        "id": "AOA-P8flLOuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "QFty3Hoc953-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing - TF-IDF Vectorization\n"
      ],
      "metadata": {
        "id": "0oLBMoF4LvOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Preprocessing - TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "C6EoxXZm_oUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data[\"text\"])\n",
        "y_train = train_data[\"label\"]"
      ],
      "metadata": {
        "id": "N7XP210t_sRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the validation and test data\n",
        "X_val_tfidf = tfidf_vectorizer.transform(val_data[\"text\"])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data[\"text\"])\n",
        "y_val = val_data[\"label\"]\n",
        "y_test = test_data[\"label\"]"
      ],
      "metadata": {
        "id": "xv2Q3QQr_uul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training - Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "2XM7wCjRL8zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "oHnpOP7_ARVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "oZcpPbqXMFZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on validation and test sets\n",
        "y_val_pred = nb_classifier.predict(X_val_tfidf)\n",
        "y_test_pred = nb_classifier.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "5E5cNrJVAUK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate performance metrics"
      ],
      "metadata": {
        "id": "6IwSP1tpMOVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred, average='weighted')  # Use 'weighted' average for multiclass classification\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='weighted')"
      ],
      "metadata": {
        "id": "Eznr8nwgAXLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validation Set Performance:\")\n",
        "print(\"Accuracy:\", val_accuracy)\n",
        "print(\"F1 Score:\", val_f1)\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(\"Accuracy:\", test_accuracy)\n",
        "print(\"F1 Score:\", test_f1)"
      ],
      "metadata": {
        "id": "sDVo7pFeAaif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis:\n",
        "\n",
        "Accuracy Evaluation: The accuracy scores for both the validation and test sets are relatively modest, indicating that the model correctly classified approximately 69% and 72% of the tweets in the respective sets.\n",
        "\n",
        "\n",
        "F1 Score Examination: The F1 scores, which consider both precision and recall, provide a more balanced assessment of the model's performance. With F1 scores of around 60% for the validation set and 63.50% for the test set, the model demonstrates moderate effectiveness in capturing the true positive and true negative cases."
      ],
      "metadata": {
        "id": "LqzqCWX6McD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improvement more lets Try Word Embedding with Word2vec with Logistic Regression"
      ],
      "metadata": {
        "id": "BGLsjwhlNakP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nxbHs3bjAc5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing - Word Embeddings with Word2Vec"
      ],
      "metadata": {
        "id": "d8atK9u5N4O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model on the training data\n",
        "sentences = [text.split() for text in train_data[\"text\"]]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "YkUazLBfDQ0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to average Word2Vec vectors for each sentence\n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.wv[word])\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "    return feature_vector"
      ],
      "metadata": {
        "id": "ic5jeYRuDVzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text data to Word2Vec embeddings\n",
        "X_train_word2vec = np.array([average_word_vectors(text.split(), word2vec_model, word2vec_model.wv.index_to_key, 100) for text in train_data[\"text\"]])\n",
        "X_val_word2vec = np.array([average_word_vectors(text.split(), word2vec_model, word2vec_model.wv.index_to_key, 100) for text in val_data[\"text\"]])\n",
        "X_test_word2vec = np.array([average_word_vectors(text.split(), word2vec_model, word2vec_model.wv.index_to_key, 100) for text in test_data[\"text\"]])\n",
        "y_train = train_data[\"label\"]\n",
        "y_val = val_data[\"label\"]\n",
        "y_test = test_data[\"label\"]"
      ],
      "metadata": {
        "id": "jp2pN5UoDbbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training - Logistic Regression"
      ],
      "metadata": {
        "id": "y9IvZ0wAN8w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the logistic regression classifier\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "logistic_regression.fit(X_train_word2vec, y_train)"
      ],
      "metadata": {
        "id": "J0eBaxlVDeB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "7ZDb4yzJOENg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on validation and test sets\n",
        "y_val_pred = logistic_regression.predict(X_val_word2vec)\n",
        "y_test_pred = logistic_regression.predict(X_test_word2vec)"
      ],
      "metadata": {
        "id": "E123lDtgDni2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate performance metrics\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred, average='weighted')  # Use 'weighted' average for multiclass classification\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='weighted')"
      ],
      "metadata": {
        "id": "5O-L7M_CDqaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validation Set Performance:\")\n",
        "print(\"Accuracy:\", val_accuracy)\n",
        "print(\"F1 Score:\", val_f1)\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(\"Accuracy:\", test_accuracy)\n",
        "print(\"F1 Score:\", test_f1)"
      ],
      "metadata": {
        "id": "3_LMRTL7DtHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis:\n",
        "\n",
        "Accuracy Assessment:\n",
        "The accuracy scores on both the validation and test sets are relatively low, with approximately 66% accuracy on the validation set and 68% accuracy on the test set. This suggests that the model correctly classified only about two-thirds of the tweets in each set, indicating a modest level of performance.\n",
        "F1 Score Evaluation:\n",
        "\n",
        "The F1 scores, which consider both precision and recall, provide a more comprehensive evaluation of the model's performance. With F1 scores of around 56% for the validation set and 58% for the test set, the model demonstrates moderate effectiveness in capturing both true positive and true negative cases."
      ],
      "metadata": {
        "id": "-d1UDXvDONet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Deployment\n",
        "\n",
        "After experimenting with various models for sentiment analysis of financial news tweets, we found that the \"classifier\" model achieved the highest accuracy. Therefore, we have decided to deploy this model for practical use."
      ],
      "metadata": {
        "id": "1_p2LAvhIxiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the trained model"
      ],
      "metadata": {
        "id": "VB1cGKJGIjEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save both the classifier and the vectorizer\n",
        "joblib.dump((classifier, vectorizer), 'sentiment_classifier.joblib')"
      ],
      "metadata": {
        "id": "GAOcZaQjDvot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Load both the classifier and the vectorizer\n",
        "classifier, vectorizer = joblib.load('sentiment_classifier.joblib')"
      ],
      "metadata": {
        "id": "9M2sZwOJEtR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}